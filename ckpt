#!/usr/bin/env python3
import os
import re

import yaml
import numpy as np
import tensorflow as tf
from docopt import docopt

import mayo
from mayo.log import log


os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

_USAGE = """
The mayo checkpoint helper.
Usage:
    ckpt <importer> <file> info
    ckpt <importer> <file> export <exporter> <to_file> [options]

Arguments:
    <importer> can be one of: {importer}.

Options:
    --dry-run           Performs a dry run, not actually changing anything
                        but shows things to be changed.
    --rules=<yaml>      Replaces keys with new keys given in the specified YAML
                        file using `re.sub`.  The YAML file should be written
                        as ordered mappings with `<pattern>: <replacement>`,
                        which we will apply the substitution in the order of
                        mapping.
"""


class BaseImporter(object):
    def __init__(self, file):
        super().__init__()
        self.var_to_shape, self.var_to_tensor = self._import(file)

    @staticmethod
    def _import(file):
        raise NotImplementedError


class NumpyImporter(BaseImporter):
    _encoding = 'ASCII'

    @classmethod
    def _import(cls, file):
        var_to_tensor = np.load(file, encoding=cls._encoding).item()
        var_to_shape = {k: list(v.shape) for k, v in var_to_tensor.items()}
        return var_to_shape, var_to_tensor


class CheckpointImporter(BaseImporter):
    @staticmethod
    def _import(file):
        reader = tf.train.NewCheckpointReader(file)
        var_to_shape = reader.get_variable_to_shape_map()
        var_to_tensor = {}
        for v in var_to_shape:
            var_to_tensor[v] = reader.get_tensor(v)
        return var_to_shape, var_to_tensor


class PyTorchImporter(BaseImporter):
    @staticmethod
    def _permute(tensor):
        """
        PyTorch tensors requires us to permute them, as the ordering is
        different from tensorflow.
        Convolution:
            torch:      [out_maps, in_maps, kernel_h, kernel_w]
            tensorflow: [kernel_h, kernel_w, in_maps, out_maps]
        FC:
            torch:      [out, in]
            tensorflow: [in, out]
        """
        dim = len(tensor.size())
        tensor = tensor.data.numpy()
        if dim == 2:
            permute = (1, 0)
        elif dim == 4:
            permute = (2, 3, 1, 0)
        else:
            return tensor
        return np.transpose(tensor, permute)

    def _import(self, file):
        from torch import load
        raw = load(file)
        var_to_shape = {}
        var_to_tensor = {}
        for k, v in raw.items():
            tensor = self._permute(v)
            var_to_tensor[k] = tensor
            var_to_shape[k] = list(tensor.shape)
        return var_to_shape, var_to_tensor


class BaseExporter(object):
    def __init__(self, var_to_tensor, rules):
        super().__init__()
        self.var_to_tensor = self._rename(var_to_tensor, rules)

    @staticmethod
    def _rename_map(variables, rules):
        vvmap = {}
        for v in variables:
            nv = v
            for pattern, replacement in rules.items():
                if replacement is None:
                    if re.findall(pattern, nv):
                        break
                else:
                    nv = re.sub(pattern, replacement, nv)
            else:
                vvmap[v] = nv
        return vvmap

    def _rename(self, var_to_tensor, rules):
        log.key('Renaming variables...')
        rename_map = self._rename_map(var_to_tensor, rules)
        new_var_to_tensor = {}
        for v in var_to_tensor:
            try:
                nv = rename_map[v]
            except KeyError:
                log.info('Skipping {!r} as it is not required'.format(v))
                continue
            if nv != v:
                log.info('Renamed {!r} as {!r}.'.format(v, nv))
            else:
                log.info('{!r} is not renamed.'.format(v))
            new_var_to_tensor[nv] = var_to_tensor[v]
        return new_var_to_tensor

    def export(self, file, dry_run=False):
        if dry_run:
            log.key('Dry run, not actually saving.')
            return
        self._export(file)

    def _export(self, file):
        raise NotImplementedError


class NumpyExporter(BaseExporter):
    def _export(self, file):
        np.save(file, self.var_to_tensor)


class CheckpointExporter(BaseExporter):
    def _export(self, file):
        session = tf.Session()
        new_vars = []
        with session.as_default():
            for var, tensor in self.var_to_tensor.items():
                new_vars.append(tf.Variable(tensor, name=var))
        log.key('Saving checkpoint with renamed variables...')
        saver = tf.train.Saver()
        session.run(tf.variables_initializer(new_vars))
        saver.save(session, file)


_importers = {
    'numpy': NumpyImporter,
    'checkpoint': CheckpointImporter,
    'torch': PyTorchImporter,
}
_exporters = {
    'numpy': NumpyExporter,
    'checkpoint': CheckpointExporter,
}


def _porter(args, porter):
    key = args['<{}porter>'.format(porter.lower())]
    omap = _importers if porter.lower() == 'im' else _exporters
    try:
        return omap[key]
    except KeyError:
        raise KeyError(
            '{}porter should be one of {!r}'
            .format(porter.capitalize(), ', '.join(omap)))


def main():
    usage = _USAGE.format(importer=', '.join(_importers))
    args = docopt(usage, version=mayo.__version__)
    importer_cls = _porter(args, 'im')
    importer = importer_cls(args['<file>'])
    if args['info']:
        print(yaml.dump(importer.var_to_shape))
    elif args['export']:
        rules = args['--rules']
        with open(rules, 'r') as f:
            rules = yaml.load(f)
        exporter_cls = _porter(args, 'ex')
        exporter = exporter_cls(importer.var_to_tensor, rules)
        exporter.export(args['<to_file>'], args['--dry-run'])
    else:
        raise TypeError('Do not know what we should do.')


if __name__ == "__main__":
    main()
