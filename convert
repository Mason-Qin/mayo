#!/usr/bin/env python3
import os
import re

import yaml
import numpy as np
from docopt import docopt

__version__ = '0.0.1'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


class Log(object):
    def log(self, text, head):
        print(head + ' ' + text)

    def key(self, text):
        self.log(text, '*')

    def info(self, text):
        self.log(text, '-')


log = Log()


class BaseImporter(object):
    def __init__(self, file, rules):
        super().__init__()
        log.key('Importing with {}...'.format(self.__class__.__name__))
        self.var_to_tensor = self._import(file, rules or {})
        self.var_to_shape = {
            k: list(v.shape) for k, v in self.var_to_tensor.items()}

    def _import(self, file, rules):
        raise NotImplementedError


class NumpyImporter(BaseImporter):
    _encoding = 'ASCII'

    @classmethod
    def _import(cls, file, rules):
        encoding = rules.get('encoding', cls._encoding)
        return np.load(file, encoding=encoding).item()


class CheckpointImporter(BaseImporter):
    @staticmethod
    def _import(file, rules):
        import tensorflow as tf
        reader = tf.train.NewCheckpointReader(file)
        var_to_tensor = {}
        for v in reader.get_variable_to_shape_map():
            var_to_tensor[v] = reader.get_tensor(v)
        return var_to_tensor


class PyTorchImporter(BaseImporter):
    def _permute(self, name, tensor, rules):
        """
        PyTorch tensors requires us to permute them, as the ordering is
        different from tensorflow.

        Convolution:
            torch:      [out_maps, in_maps, kernel_h, kernel_w]
            tensorflow: [kernel_h, kernel_w, in_maps, out_maps]
        FC:
            torch:      [out, in]
            tensorflow: [in, out]

        Additionally, because the ordering of inputs are different between
        TensorFlow [height, width, channels] and PyTorch [channels, height,
        width], flatten + FC requires additional processing.  For instance,
        AlexNet fc6 in PyTorch uses 4096x9216, where 9216 is flattened
        from 256x6x6.  In order to produce the correct spatial ordering in
        TensorFlow, we need to first unflatten 9216x4096 into 256x6x6x4096,
        permute into 4096x6x6x256 and reflatten it to 4096x9216 and finally
        permute into 9216x4096.
        """
        shape = list(tensor.size())
        dim = len(shape)
        permute_order = {
            2: (1, 0),
            4: (2, 3, 1, 0),
        }
        order = permute_order.get(dim)
        tensor = tensor.data.numpy()
        if not order:
            return tensor
        unflat_shape = rules.get(name)
        if unflat_shape:
            log.info(
                'Unflattening {!r} with shape {} into {}...'
                .format(name, shape, unflat_shape))
            tensor = np.reshape(tensor, unflat_shape)
            tensor = np.transpose(tensor, (0, 2, 3, 1))
            log.info(
                'Reflattening permuted tensor of shape {} into {}...'
                .format(list(tensor.shape), shape))
            tensor = np.reshape(tensor, shape)
        log.info('Permuted {!r} of shape {!r}.'.format(name, shape))
        return np.transpose(tensor, order)

    def _import(self, file, rules):
        from torch import load
        raw = load(file)
        var_to_tensor = {}
        for k, v in raw.items():
            tensor = self._permute(k, v, rules.get('unflat', {}))
            var_to_tensor[k] = tensor
        return var_to_tensor


class BaseExporter(object):
    def __init__(self, var_to_tensor, rules):
        super().__init__()
        rules = (rules or {}).get('rename')
        if rules:
            self.var_to_tensor = self._rename(var_to_tensor, rules)
        else:
            self.var_to_tensor = var_to_tensor

    @staticmethod
    def _rename_map(variables, rules):
        vvmap = {}
        for v in variables:
            nv = v
            for pattern, replacement in rules.items():
                if replacement is None:
                    if re.findall(pattern, nv):
                        break
                else:
                    nv = re.sub(pattern, replacement, nv)
            else:
                vvmap[v] = nv
        return vvmap

    def _rename(self, var_to_tensor, rules):
        log.key('Renaming variables...')
        rename_map = self._rename_map(var_to_tensor, rules)
        new_var_to_tensor = {}
        for v in var_to_tensor:
            try:
                nv = rename_map[v]
            except KeyError:
                log.info('Skipping {!r} as it is not required'.format(v))
                continue
            if nv != v:
                log.info('Renamed {!r} as {!r}.'.format(v, nv))
            else:
                log.info('{!r} is not renamed.'.format(v))
            new_var_to_tensor[nv] = var_to_tensor[v]
        return new_var_to_tensor

    def export(self, file, dry_run=False):
        if dry_run:
            log.key('Dry run, not actually saving.')
            return
        log.key('Exporting with {}...'.format(self.__class__.__name__))
        self._export(file)

    def _export(self, file):
        raise NotImplementedError


class NumpyExporter(BaseExporter):
    def _export(self, file):
        np.save(file, self.var_to_tensor)


class CheckpointExporter(BaseExporter):
    def _export(self, file):
        import tensorflow as tf
        session = tf.Session()
        new_vars = []
        log.info('Instantiating variables...')
        with session.as_default():
            for var, tensor in self.var_to_tensor.items():
                new_vars.append(tf.Variable(tensor, name=var))
        log.info('Saving checkpoint with renamed variables...')
        saver = tf.train.Saver()
        session.run(tf.variables_initializer(new_vars))
        saver.save(session, file)


_importers = {
    '': CheckpointImporter,
    '.npy': NumpyImporter,
    '.pth': PyTorchImporter,
}
_exporters = {
    '': CheckpointExporter,
    '.npy': NumpyExporter,
}


_USAGE = """
The universal deep learning framework saved model converter.
Usage:
    {executable} <from_file> info [options]
    {executable} <from_file> to <to_file> [options]

Arguments:
    * <from_file> can be:
{importers}
    * <to_file> can be:
{exporters}

Options:
    --dry-run           Performs a dry run, not actually changing anything
                        but shows things to be changed.
    --rules=<yaml>      Replaces keys with new keys given in the specified YAML
                        file using `re.sub`.  The YAML file should be written
                        as follows:
                            rename: <ordered mapping>
                        Here, <ordered mapping> is filled with
                            `<pattern>: <replacement>`
                        which we will apply the substitution in the order of
                        mapping.  The file could contain additional information
                        required by specific importer and exporter processing,
                        such as `unflat` in `PyTorchImporter`.
"""


def usage():
    def format_porters(porters):
        descs = []
        for index, (suffix, porter) in enumerate(porters.items()):
            if not suffix:
                desc = 'a file without a suffix'
            else:
                desc = 'a {!r} file'.format(suffix)
            final = ';'
            if index == len(porters) - 2:
                final = '; or'
            elif index == len(porters) - 1:
                final = '.'
            descs.append(
                '{}- {} ({}){}'.format(' ' * 8, desc, porter.__name__, final))
        return '\n'.join(descs)

    importers = format_porters(_importers)
    exporters = format_porters(_exporters)
    return _USAGE.format(
        executable=__file__, importers=importers, exporters=exporters)


def _porter(file, porters):
    name, suffix = os.path.splitext(file)
    try:
        return porters[suffix]
    except KeyError:
        raise ValueError('Unrecognized suffix {!r}'.format(suffix))


def main():
    args = docopt(usage(), version=__version__)
    from_file = args['<from_file>']
    rules = args['--rules']
    if rules:
        with open(rules, 'r') as f:
            rules = yaml.load(f)
    importer_cls = _porter(from_file, _importers)
    importer = importer_cls(from_file, rules)
    if args['info']:
        print(yaml.dump(importer.var_to_shape))
    elif args['to']:
        to_file = args['<to_file>']
        exporter_cls = _porter(to_file, _exporters)
        exporter = exporter_cls(importer.var_to_tensor, rules)
        exporter.export(to_file, args['--dry-run'])
    else:
        raise TypeError('Do not know what we should do.')


if __name__ == "__main__":
    main()
